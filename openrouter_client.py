"""
OpenRouter API Client для PDF Chat Assistant

Клиент для интеграции с OpenRouter API для генерации ответов от различных LLM моделей.
Поддерживает множественные модели и создание контекстных промптов с RAG.

Основные функции:
- Интеграция с OpenRouter API
- Поддержка множественных LLM моделей
- Создание контекстных промптов с найденными фрагментами
- Управление токенами и ограничениями API
"""

import requests
import json
from typing import List, Dict, Any, Optional

class OpenRouterClient:
    def __init__(self, api_key: str, base_url: str = "https://openrouter.ai/api/v1"):
        """
        Initialize OpenRouter API client.
        
        Args:
            api_key: OpenRouter API key
            base_url: Base URL for OpenRouter API
        """
        self.api_key = api_key
        self.base_url = base_url
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://streamlit.io",  # Optional: for analytics
            "X-Title": "PDF Chat Assistant"  # Optional: for analytics
        }
    
    def get_response(
        self, 
        messages: List[Dict[str, str]], 
        question: str, 
        context: str, 
        model: str = "openai/gpt-3.5-turbo",
        max_tokens: int = 1000,
        temperature: float = 0.7,
        images: List[str] = None
    ) -> str:
        """
        Get response from OpenRouter API using RAG context.
        
        Args:
            messages: Previous conversation messages
            question: Current user question
            context: Retrieved context from PDF
            model: Model identifier
            max_tokens: Maximum tokens in response
            temperature: Response creativity (0.0 to 1.0)
            
        Returns:
            Generated response text
        """
        try:
            # Prepare system prompt with context
            system_prompt = self._create_system_prompt(context)
            
            # Prepare messages for the API
            api_messages = [{"role": "system", "content": system_prompt}]
            
            # Add conversation history (limit to last 10 messages to avoid token limits)
            recent_messages = messages[-10:] if len(messages) > 10 else messages
            api_messages.extend(recent_messages)
            
            # Add current question
            api_messages.append({"role": "user", "content": question})
            
            # Prepare request payload
            payload = {
                "model": model,
                "messages": api_messages,
                "max_tokens": max_tokens,
                "temperature": temperature,
                "stream": False
            }
            
            # Make API request
            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers=self.headers,
                json=payload,
                timeout=60
            )
            
            # Handle API errors
            if response.status_code != 200:
                error_detail = self._parse_error_response(response)
                raise Exception(f"OpenRouter API error ({response.status_code}): {error_detail}")
            
            # Parse response
            response_data = response.json()
            
            if 'choices' not in response_data or not response_data['choices']:
                raise Exception("No response generated by the model")
            
            # Extract and return the response text
            assistant_response = response_data['choices'][0]['message']['content']
            
            return assistant_response.strip()
            
        except requests.exceptions.RequestException as e:
            raise Exception(f"Network error: {str(e)}")
        except json.JSONDecodeError as e:
            raise Exception(f"Failed to parse API response: {str(e)}")
        except Exception as e:
            raise Exception(f"Error getting response from OpenRouter: {str(e)}")
    
    def _create_system_prompt(self, context: str) -> str:
        """
        Create system prompt with PDF context.
        
        Args:
            context: Retrieved context from PDF
            
        Returns:
            System prompt string
        """
        return f"""You are a helpful AI assistant that answers questions about a PDF document. 

Here is the relevant context from the PDF document:

{context}

Instructions:
1. Answer questions based primarily on the provided context from the PDF
2. If the answer is not in the context, clearly state that the information is not available in the provided document
3. Be accurate and cite specific information from the context when possible
4. If asked about something not related to the PDF content, politely redirect the conversation back to the document
5. Provide clear, concise, and helpful answers
6. If you need clarification about the question, ask for it

Remember: Your primary role is to help users understand and extract information from their uploaded PDF document."""
    
    def _parse_error_response(self, response: requests.Response) -> str:
        """
        Parse error response from OpenRouter API.
        
        Args:
            response: HTTP response object
            
        Returns:
            Error message string
        """
        try:
            error_data = response.json()
            if 'error' in error_data:
                error = error_data['error']
                if isinstance(error, dict):
                    return error.get('message', str(error))
                return str(error)
            return response.text
        except:
            return f"HTTP {response.status_code}: {response.reason}"
    
    def get_available_models(self) -> List[Dict[str, Any]]:
        """
        Get list of available models from OpenRouter.
        
        Returns:
            List of model dictionaries
        """
        try:
            response = requests.get(
                f"{self.base_url}/models",
                headers=self.headers,
                timeout=30
            )
            
            if response.status_code != 200:
                raise Exception(f"Failed to fetch models: {response.status_code}")
            
            return response.json().get('data', [])
            
        except Exception as e:
            print(f"Error fetching models: {e}")
            return []
    
    def test_connection(self) -> bool:
        """
        Test connection to OpenRouter API.
        
        Returns:
            True if connection is successful, False otherwise
        """
        try:
            response = requests.get(
                f"{self.base_url}/models",
                headers=self.headers,
                timeout=10
            )
            return response.status_code == 200
        except:
            return False
    
    def estimate_tokens(self, text: str) -> int:
        """
        Rough estimation of token count for text.
        This is a simple approximation (actual tokenization varies by model).
        
        Args:
            text: Text to estimate tokens for
            
        Returns:
            Estimated token count
        """
        # Rough approximation: ~4 characters per token for English text
        return len(text) // 4
    
    def truncate_context(self, context: str, max_tokens: int = 3000) -> str:
        """
        Truncate context to fit within token limits.
        
        Args:
            context: Context text to truncate
            max_tokens: Maximum token limit
            
        Returns:
            Truncated context
        """
        estimated_tokens = self.estimate_tokens(context)
        
        if estimated_tokens <= max_tokens:
            return context
        
        # Calculate approximate character limit
        char_limit = max_tokens * 4
        
        if len(context) <= char_limit:
            return context
        
        # Truncate and try to break at sentence boundary
        truncated = context[:char_limit]
        
        # Find last sentence ending
        last_sentence = max(
            truncated.rfind('.'),
            truncated.rfind('!'),
            truncated.rfind('?')
        )
        
        if last_sentence > char_limit * 0.8:  # If we found a good break point
            return truncated[:last_sentence + 1]
        
        return truncated + "..."
